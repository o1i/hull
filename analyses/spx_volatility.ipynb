{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPX volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to investigate the volatility of SPX and to see whether the measured volatility matches the implied volatilities observed through options. The following steps will be undertaken:\n",
    "\n",
    "\n",
    "- **Day count**: I investigate whether non-trading days have lower volatility than trading days and whether changes on the last trading day before a non trading day have a different volatility than other trading days (also based on a comment in the book)\n",
    "- **Long history**: I compare different moving average windows in order to see what can be said about the recommended window of 90 or 180 days\n",
    "- **Volatility growth**: In many (risk neutral!) calculations, uncertainty grows as `sqrt(t)`. This is compared with the (real world) increase in volatilities. Since the volatiliy does not change from the real world to risk neutral, this relationship should also hold on observed data.\n",
    "- **Implied volatilities**: A simple comparison between implied and (backward-)measured volatilities.\n",
    "- **Volatility smiles**: A comparison of implied volatilities for different strikes to compare against the expectation from the book.\n",
    "\n",
    "A version of the notebook is available as html file since github sometimes cannot properly display notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline.offline import iplot\n",
    "from scipy.stats import spearmanr, norm\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, RANSACRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day count conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = requests.get(\"https://raw.githubusercontent.com/o1i/hull/main/data/2012-12-13_spx_historic.csv\").content.decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_hist = pd.read_csv(StringIO(csv))\n",
    "dt_fmt = \"%Y-%m-%d\"\n",
    "spx_hist[\"date_dt\"] = spx_hist[\"date\"].map(lambda x: datetime.strptime(x, dt_fmt))\n",
    "spx_hist.sort_values(\"date_dt\", inplace=True)\n",
    "spx_hist.set_index(\"date_dt\", inplace=True)\n",
    "spx_hist[\"weekday\"] = spx_hist.index.map(lambda x: x.strftime(\"%a\"))\n",
    "spx_hist[\"log_return\"] = np.log10(spx_hist[\"close\"] / spx_hist[\"close\"].shift(1))\n",
    "\n",
    "# The first 15 years or so open = close --> to be excluded\n",
    "first_close_unlike_open = list(~(spx_hist[\"open\"] == spx_hist[\"close\"])).index(True)\n",
    "spx_hist_short = spx_hist[first_close_unlike_open:]\n",
    "intra_day = np.log10(spx_hist_short[\"close\"] / spx_hist_short[\"open\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra Day moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout_yaxis_range=[-0.01,0.01], \n",
    "                layout_title=\"One-day log10-returns by weekday\")\n",
    "for wd in [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]:\n",
    "    fig.add_trace(go.Box(y=intra_day[spx_hist_short[\"weekday\"] == wd], name=wd))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the median values are increasing over the week (aka relatively more positive movements towards the end of the week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout_yaxis_range=[0,0.01], \n",
    "                layout_title=\"Absolute one day log10 returns by weekday\")\n",
    "for wd in [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]:\n",
    "    fig.add_trace(go.Box(y=intra_day[spx_hist_short[\"weekday\"] == wd].map(lambda x: abs(x)), name=wd))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While both Q3 as well as the upper fence is lower on Friday, it does not seem to fundamentally change the picture compared to other days. Also assuming that this pattern is so trivial it would be exploited until it no longer was a pattern, I will not treat Fridays differently in what follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trading vs non-trading days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the data available to me the \"implied\" prices at the end of non-business days were not availabe, I will compare the following:\n",
    "\n",
    "- The close of day `d` is compared to the open of `d+3` for Mondays, Tuesdays and Fridays.\n",
    "- Only two-day breaks over the weekend will be considered for simplicity. Any three-day weekend or a non-trading day in the middle of the week will be ignored.\n",
    "- Only the pattern over the entire period is analysed. Changes in this behaviour could be academically of interest, but not done here since not at the heart of what this notebook should be (implied volatilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaks = spx_hist_short.copy()\n",
    "breaks[[\"wd_1\", \"open_1\"]] = breaks[[\"weekday\", \"open\"]].shift(-1)\n",
    "breaks[[\"wd_3\", \"open_3\"]] = breaks[[\"weekday\", \"open\"]].shift(-3)\n",
    "breaks = breaks[((breaks[\"weekday\"] == \"Mon\") & (breaks[\"wd_3\"] == \"Thu\")) | \n",
    "                ((breaks[\"weekday\"] == \"Tue\") & (breaks[\"wd_3\"] == \"Fri\")) |\n",
    "                ((breaks[\"weekday\"] == \"Fri\") & (breaks[\"wd_1\"] == \"Mon\"))]\n",
    "breaks[\"open_after\"] = np.where(breaks[\"weekday\"] == \"Fri\", breaks[\"open_1\"], breaks[\"open_3\"])\n",
    "gap = np.log10(breaks[\"open_after\"] / breaks[\"close\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout_yaxis_range=[-0.03, 0.03], \n",
    "                layout_title=\"log10(open_(d+2) / close(d)) starting on different weekdays\")\n",
    "for wd in [\"Mon\", \"Tue\", \"Fri\"]:\n",
    "    fig.add_trace(go.Box(y=gap[breaks[\"weekday\"] == wd], name=wd))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is significantly more movement over trading periods than in non-trading periods. I will therefore, as suggested by Hull, ignore non-trading days but treat fridays as any other day. Thus the holes in the time series do not require special treatment. Just as a confirmation, I will look at close-to-close variability that now should be slightly larger for mondays that incorporate the small Friday close to Monday open volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout_yaxis_range=[0,0.025], \n",
    "                layout_title=\"Close-to-close absolute 1-day backward looking log10-returns for consecutive trading days\")\n",
    "for wd in [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]:\n",
    "    fig.add_trace(go.Box(y=spx_hist_short.loc[spx_hist_short[\"weekday\"] == wd, \"log_return\"].map(lambda x: abs(x)), name=wd))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the values are a tad higher, but by surprisingly little.\n",
    "\n",
    "What is not done here is to see whether on bank holidays (which may be idiosyncratic to U.S. stocks) there is more volatility than on weekends (that are the same in most major market places). One hypothesis could be that the reduced volatility is due to less information on those days, which would be more the case for weekends than for country-specific days off.\n",
    "\n",
    "Since we can now look at close to close movements, the whole time series becomes usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past volatility to predict future volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption: 252 business days per year, i.e. 21 per month\n",
    "def std_trace(n_month: int, col: str, name: str, backward: bool = True):\n",
    "    n = 21*n_month\n",
    "    window = n if backward else pd.api.indexers.FixedForwardWindowIndexer(window_size=n)\n",
    "    return go.Scatter(\n",
    "        x=spx_hist.iloc[::5].index,\n",
    "        y=spx_hist[\"log_return\"].rolling(window).std().values[::5],\n",
    "        mode=\"lines\",\n",
    "        marker={\"color\":col},\n",
    "        name=name,\n",
    "        text=[f\"Index: {i}\" for i in range(len(spx_hist.index))]\n",
    "    )\n",
    "#trace_bw_1m = std_trace(1, \"#762a83\", \"BW 1m\", True)\n",
    "trace_bw_3m = std_trace(3, \"#9970ab\", \"BW 3m\", True)\n",
    "trace_bw_6m = std_trace(6, \"#c2a5cf\", \"BW 6m\", True)\n",
    "#trace_bw_12m = std_trace(12, \"#e7d4e8\", \"BW 12m\", True)\n",
    "#trace_fw_1m = std_trace(1, \"#1b7837\", \"FW 1m\", False)\n",
    "trace_fw_3m = std_trace(3, \"#5aae61\", \"FW 3m\", False)\n",
    "trace_fw_6m = std_trace(6, \"#a6dba0\", \"FW 6m\", False)\n",
    "#trace_fw_12m = std_trace(12, \"#d9f0d3\", \"FW 12m\", False)\n",
    "\n",
    "layout = {\n",
    "    'showlegend': True,\n",
    "    \"title\": \"Little agreement of backward and forward standard deviation\",\n",
    "    \"xaxis\": {\"title\": \"Date\"},\n",
    "    \"yaxis\": {\"title\": \"Std of daily close-to-close log-returns\"}\n",
    "}\n",
    "\n",
    "fig = {\n",
    "    'layout': layout,\n",
    "    'data': [#trace_bw_1m, \n",
    "             trace_bw_3m, \n",
    "             trace_bw_6m, \n",
    "             #trace_bw_12m,\n",
    "             #trace_fw_1m, \n",
    "             trace_fw_3m, \n",
    "             trace_fw_6m, \n",
    "             #trace_fw_12m\n",
    "    ],\n",
    "}\n",
    "\n",
    "iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as if except in the stationary case (ca 2012-2015) the past volatility does a surprisingly bad job of predicting future volatility (with obvious implications for options pricing). While one could do formal statistical tests, I believe a scatterplot and maybe an R2 or so will get me closer to a feeling about what is actually happening.\n",
    "\n",
    "All four trailing windows can be used as estimators for all the leading windows, leading to 16 possible combinations. Also, these windows are available on every trading day and therefore looking at windows on every day would lead to strong dependencies whereas arbitrarily choosing how to split the data into disjunct parts may also lead to variance inflation.\n",
    "\n",
    "I will therefore for one example (6m back, 6m forward) compare the variance of the R2 estimator introduced by the choice of windows, and if sufficiently small pick the canonical non-overlapping windowms for every combination of leading and trailing window size for further analysis. The expectation is that the plot of offset vs R2 is nearly constant and has (almost) the same values for offset 0 as for offset 252-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(252/2)\n",
    "backward = spx_hist[\"log_return\"].rolling(n).std()\n",
    "forward = spx_hist[\"log_return\"].rolling(pd.api.indexers.FixedForwardWindowIndexer(window_size=n)).std()\n",
    "valid = backward.notna() & forward.notna()\n",
    "backward = backward[valid]\n",
    "forward = forward[valid]\n",
    "index = np.array(range(len(forward)))\n",
    "\n",
    "def get_r2(offset: int, window: int):\n",
    "    x = backward[offset::window].values.reshape([-1, 1])\n",
    "    y = forward[offset::window]\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    return model.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 252\n",
    "fig = go.Figure(layout_yaxis_range=[0,0.5], \n",
    "                layout_title=\"Expanatory power measured in R2 depends heavily on window offset\",\n",
    "                layout_xaxis_title=\"Offset (in trading days)\",\n",
    "                layout_yaxis_title=\"R2 of forward std regressed on backward std\")\n",
    "fig.add_trace(go.Scatter(x=list(range(window)), y=[get_r2(i, window) for i in range(window)], mode=\"markers+lines\"))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly only the second assumption holds. It appears as if R2 is extremely sensitive to the offset. For example, comparing offset 0 vs 50 R2 drops from about 50% to 10% explained variance, which would mean that deciding on a backwards window size to predict a certain future window of volatility would have to somehow take into account all possible offsets. To confirm, let's have a closer look at this specific example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 252\n",
    "offset_0 = 0\n",
    "offset_1 = 50\n",
    "x0 = backward[offset_0::window]\n",
    "y0 = forward[offset_0::window]\n",
    "x1 = backward[offset_1::window]\n",
    "y1 = forward[offset_1::window]\n",
    "text_0 = [f\"Index: {offset_0 + i * window}, bw: {x0_}, fw: {y0[i]}\" for i, x0_ in enumerate(x0)]\n",
    "text_1 = [f\"Index: {offset_1 + i * window}, bw: {x1_}, fw: {y1[i]}\" for i, x1_ in enumerate(x1)]\n",
    "\n",
    "min_x = min(min(x0), min(x1))\n",
    "max_x = max(max(x0), max(x1))\n",
    "\n",
    "m0 = LinearRegression()\n",
    "m0.fit(x0.values.reshape([-1, 1]), y0)\n",
    "\n",
    "m1 = LinearRegression()\n",
    "m1.fit(x1.values.reshape([-1, 1]), y1)\n",
    "\n",
    "fig = go.Figure(layout_title=\"Comparable dispersion despide large R2-difference for offsets 0 and 50\",\n",
    "                layout_xaxis_title=\"Backward standard deviation\",\n",
    "                layout_yaxis_title=\"Forward standard deviation\")\n",
    "fig.add_trace(go.Scatter(x=x0, y=y0, mode=\"markers\", name=f\"Offset {offset_0}\", marker={\"color\": \"#1f77b4\"},\n",
    "                         text=text_0))\n",
    "fig.add_trace(go.Scatter(x=x1, y=y1, mode=\"markers\", name=f\"Offset {offset_1}\", marker={\"color\": \"#ff7f0e\"},\n",
    "                        text=text_1))\n",
    "fig.add_trace(go.Scatter(x=[min_x, max_x], y=[min_x, max_x], \n",
    "                         line={\"color\": \"#aaaaaa\"}, name=\"1:1-line\", mode=\"lines\"))\n",
    "fig.add_trace(go.Scatter(x=[min_x, max_x], y=[m0.intercept_ + m0.coef_[0] * min_x, m0.intercept_ + m0.coef_[0] * max_x], \n",
    "                         line={\"color\": \"#1f77b4\",  \"dash\":\"dash\"}, mode=\"lines\", showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=[min_x, max_x], y=[m1.intercept_ + m1.coef_[0] * min_x, m1.intercept_ + m1.coef_[0] * max_x], \n",
    "                         line={\"color\": \"#ff7f0e\",  \"dash\":\"dash\"}, mode=\"lines\", showlegend=False))\n",
    "iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes clear that pearson correlation may not be an ideal choice for this kind of analysis. When looking at tho two sets of points, the dispersion seems comparable and I am convinced that the outliers dominate the residual sums of squares. So probably a more robust measure of correlation may improve things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sr2(offset: int, window: int):\n",
    "    return spearmanr(backward[offset::window], forward[offset::window]).correlation\n",
    "\n",
    "window = 252\n",
    "fig = go.Figure(layout_yaxis_range=[0,1], \n",
    "                layout_title=\"Spearmans rho less sensitive to window offset than R2\",\n",
    "                layout_xaxis_title=\"Offset (in trading days)\",\n",
    "                layout_yaxis_title=\"Spearman's rho\")\n",
    "fig.add_trace(go.Scatter(x=list(range(window)), y=[get_sr2(i, window) for i in range(window)], mode=\"markers+lines\"))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement that the choice of window offset does not impact further analysis is not correct. If standard OLS is used to choose the best backward window size to predict the volatility in the future one may incur significant distortions depending on the window used.\n",
    "\n",
    "However, the statement that the choice of window offset has a significant impact on the predictive power seems equally tenuous, since the dispersion (if measured using rank correlations) is fairly stable.\n",
    "\n",
    "The problems arise with large spikes in volatility that seem to be unpredictable as well as short-lived. Neither ignoring them (R2-Problem) nor deleting those data seems to be a good option. Instead I propose to use a more robust regression.\n",
    "\n",
    "I will consider RANSAC and Huber regression, choosing the one with less volatility of the parameters over time (and again, if this were a real exercise, the same would have to be done for all combinations of forward and backward windows to ensure that the finding is not an artifact of the one pair chosen for this analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 252\n",
    "huber = HuberRegressor()\n",
    "ransac = RANSACRegressor()\n",
    "ols = LinearRegression()\n",
    "\n",
    "def get_parameters(offset: int, window: int) -> tuple:\n",
    "    \"\"\"Return Huber-intercept, Huber beta, RANSAC-intercept and RANSAC-beta\"\"\"\n",
    "    x = backward[offset::window].values.reshape([-1, 1])\n",
    "    y = forward[offset::window]\n",
    "    huber.fit(x, y)\n",
    "    ransac.fit(x, y)\n",
    "    ols.fit(x, y)\n",
    "    return np.array([huber.intercept_, huber.coef_[0], \n",
    "                     ransac.estimator_.intercept_, ransac.estimator_.coef_[0],\n",
    "                     ols.intercept_, ols.coef_[0]]).reshape([1, -1])\n",
    "\n",
    "coefs = np.concatenate([get_parameters(i, window) for i in range(window)])\n",
    "\n",
    "fig = go.Figure(layout_yaxis_range=[0,1], \n",
    "                layout_title=\"Huber Regression more stable, but still with significant variability\",\n",
    "                layout_xaxis_title=\"Offset (in trading days)\",\n",
    "                layout_yaxis_title=\"Coefficients\")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 0], line={\"color\": \"#1f77b4\", \"dash\":\"dash\"}, name=\"Intercept Huber\"))\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 1], line={\"color\": \"#1f77b4\", \"dash\":\"solid\"}, name=\"Coef Huber\"))\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 2], line={\"color\": \"#7f7f7f\", \"dash\":\"dash\"}, name=\"Intercept RANSAC\"))\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 3], line={\"color\": \"#7f7f7f\", \"dash\":\"solid\"}, name=\"Coef RANSAC\"))\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 4], line={\"color\": \"#2ca02c\", \"dash\":\"dash\"}, name=\"Intercept OLS\"))\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 5], line={\"color\": \"#2ca02c\", \"dash\":\"solid\"}, name=\"Coef OLS\"))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the (highly volatile) RANSAC, I am somewhat surprised at the fact that the outliers affect the parameters of the regression to a lesser degree than the window offset. Also it is noteworthy that the coefficient is markedly lower than 1 for most windows which is somewhat at odds with the expectations. One explanation could be the spikedness of high-volatility phases: In phases where backwards-volatility is particularly high, the forward volatility is lower than the backward volatility which would explain the size of the coefficient being less than one.\n",
    "\n",
    "To come to a conclusion about finding \"the best\" way of predicting the volatility in a given future window: Parameters based on linear estimates between forward and backward volatilities are highly dependent on the window offset and it is not obvious how to choose a point estimator this way. An obvious solution would be to allow for some non-linear dependency between the backwards volatility and the forward volatility. One way would be to apply a log transform to the predictor, another would be to add polynomial terms. Let's try both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 252\n",
    "offset_0 = 0\n",
    "offset_1 = 171\n",
    "x0 = backward[offset_0::window].values.reshape([-1, 1])\n",
    "y0 = forward[offset_0::window]\n",
    "x1 = backward[offset_1::window].values.reshape([-1, 1])\n",
    "y1 = forward[offset_1::window]\n",
    "\n",
    "all_x = np.concatenate([x0, x1])\n",
    "min_x = all_x.min()\n",
    "max_x = all_x.max()\n",
    "x_pred = np.linspace(min_x, max_x, 200).reshape([-1, 1])\n",
    "\n",
    "poly_trafo = PolynomialFeatures(degree=4)\n",
    "\n",
    "m = LinearRegression()\n",
    "\n",
    "m.fit(poly_trafo.fit_transform(x0), y0)\n",
    "m0_pred_poly = m.predict(poly_trafo.fit_transform(x_pred))\n",
    "m.fit(np.log(x0), y0)\n",
    "m0_pred_log = m.predict(np.log(x_pred))\n",
    "\n",
    "m.fit(poly_trafo.fit_transform(x1), y1)\n",
    "m1_pred_poly = m.predict(poly_trafo.fit_transform(x_pred))\n",
    "m.fit(np.log(x1), y1)\n",
    "m1_pred_log = m.predict(np.log(x_pred))\n",
    "\n",
    "\n",
    "\n",
    "col_0 = \"#1f77b4\"\n",
    "col_1 = \"#ff7f0e\"\n",
    "\n",
    "fig = go.Figure(layout_title=\"Comparable dispersion despide large R2-difference for offsets 0 and 50\",\n",
    "               layout_xaxis_title=\"Backward standard deviation\",\n",
    "               layout_yaxis_title=\"Forward standard deviation\",\n",
    "               layout_yaxis_range=[0,0.015])\n",
    "fig.add_trace(go.Scatter(x=x0.flatten(), y=y0, mode=\"markers\", name=f\"Offset {offset_0}\", marker={\"color\": col_0},\n",
    "                        text=text_0))\n",
    "fig.add_trace(go.Scatter(x=x1.flatten(), y=y1, mode=\"markers\", name=f\"Offset {offset_1}\", marker={\"color\": col_1},\n",
    "                       text=text_1))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[min_x, max_x], y=[min_x, max_x],\n",
    "                        line={\"color\": \"#aaaaaa\"}, name=\"1:1-line\", mode=\"lines\"))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), \n",
    "                         y=m0_pred_poly,\n",
    "                         line={\"color\": col_0,  \"dash\":\"dash\"}, mode=\"lines\", name=\"Polynomial\"))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), \n",
    "                        y=m0_pred_log, \n",
    "                        line={\"color\": col_0,  \"dash\":\"dot\"}, mode=\"lines\", name=\"Log trafo\"))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), \n",
    "                         y=m1_pred_poly,\n",
    "                         line={\"color\": col_1,  \"dash\":\"dash\"}, mode=\"lines\", name=\"Polynomial\"))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), \n",
    "                        y=m1_pred_log, \n",
    "                        line={\"color\": col_1,  \"dash\":\"dot\"}, mode=\"lines\", name=\"Log trafo\"))\n",
    "\n",
    "\n",
    "iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, polynomial fits behave unpredictably towards outliers and the comparison of how strong coefficients react to window offsets will only be done for the (Huberised) log-transformed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 252\n",
    "huber = HuberRegressor()\n",
    "huber2 = HuberRegressor()\n",
    "\n",
    "def get_parameters_log(offset: int, window: int) -> tuple:\n",
    "    \"\"\"Return Huber-intercept, Huber beta, RANSAC-intercept and RANSAC-beta\"\"\"\n",
    "    x = backward[offset::window].values.reshape([-1, 1])\n",
    "    y = forward[offset::window]\n",
    "    huber.fit(x, y)\n",
    "    huber2.fit(np.log(x), y)\n",
    "    return np.array([huber.intercept_, huber.coef_[0], \n",
    "                     huber2.intercept_, huber2.coef_[0], ]).reshape([1, -1])\n",
    "\n",
    "coefs = np.concatenate([get_parameters_log(i, window) for i in range(window)])\n",
    "\n",
    "fig = go.Figure(layout_title=\"Parameters of model on transformed data less volatile in absolute terms\",\n",
    "                layout_xaxis_title=\"Offset (in trading days)\",\n",
    "                layout_yaxis_title=\"Coefficients\")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 0], line={\"color\": col_0, \"dash\":\"dash\"}, name=\"Intercept Untransformed\"))\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 1], line={\"color\": col_0, \"dash\":\"solid\"}, name=\"Coef Untransformed\"))\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 2], line={\"color\": col_1, \"dash\":\"dash\"}, name=\"Intercept Transformed\"))\n",
    "fig.add_trace(go.Scatter(x=np.arange(window), y=coefs[:, 3], line={\"color\": col_1, \"dash\":\"solid\"}, name=\"Coef Transformed\"))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in absolute terms the fluctuations of the parameters did not change by much, in relative terms the situation did not get much better. However, maybe this was the wrong way of looking at the problem: While from a modelling point of view (and for the confidence in the model) it would of course be very desirable to have stable model parameters, maybe in practice the stability of the results are more important. As a last analysis before actually finding a good choice of window to predict a given future volatility, I will look at the variability of prediction depending on the window offset.\n",
    "\n",
    "For that I will outline an area marking the interquartile range as well as lines for the median and the 5% and 95% quantiles for every point for which I predict both untransformed and log-transformed inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 252\n",
    "huber = HuberRegressor()\n",
    "huber2 = HuberRegressor()\n",
    "x_pred = np.linspace(min(min(forward), min(backward)), max(max(forward), max(backward)), 200).reshape([-1, 1])\n",
    "\n",
    "\n",
    "\n",
    "def get_parameters_log(offset: int, window: int) -> tuple:\n",
    "    \"\"\"Return Huber-intercept, Huber beta, RANSAC-intercept and RANSAC-beta\"\"\"\n",
    "    x = backward[offset::window].values.reshape([-1, 1])\n",
    "    y = forward[offset::window]\n",
    "    huber.fit(x, y)\n",
    "    untransformed = huber.predict(x_pred).reshape([-1, 1, 1]) # Dims: x, offset, model\n",
    "    huber2.fit(np.log(x), y)\n",
    "    transformed = huber2.predict(np.log(x_pred)).reshape([-1, 1, 1])\n",
    "    return np.concatenate([untransformed, transformed], axis = 2)\n",
    "\n",
    "preds = np.concatenate([get_parameters_log(i, window) for i in range(window)], axis=1)\n",
    "quantiles = np.quantile(preds, [0.05, 0.25, 0.5, 0.75, 0.95], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_obs = np.linspace(x_pred.min(), x_pred.max(), 30)\n",
    "bins = np.digitize(backward, x_obs)\n",
    "observed = (pd.DataFrame({\"bin\": bins, \"fw\": forward})\n",
    "            .groupby(\"bin\")[\"fw\"]\n",
    "            .quantile(q=[0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "            .unstack(level=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_0 = \"rgba(31,119,180, 0.2)\"\n",
    "col_1 = \"rgba(255,127,14, 0.2)\"\n",
    "gray = \"rgba(70, 70, 70, 0.2)\"\n",
    "\n",
    "fig = go.Figure(layout_title=\"Overall fit is hard to judge\",\n",
    "                layout_xaxis_title=\"Past volatility\",\n",
    "                layout_yaxis_title=\"Predicted future volatility\")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_obs, y=observed[0.05].values, line={\"color\": gray, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_obs, y=observed[0.50].values, line={\"color\": gray, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_obs, y=observed[0.95].values, line={\"color\": gray, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_obs, y=observed[0.25].values, line={\"color\": gray, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_obs, y=observed[0.75].values, line={\"color\": gray, \"dash\":\"solid\"}, name=\"Observed\", fill=\"tonexty\"))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[0, :, 0], line={\"color\": col_0, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[2, :, 0], line={\"color\": col_0, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[4, :, 0], line={\"color\": col_0, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[1, :, 0], line={\"color\": col_0, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[3, :, 0], line={\"color\": col_0, \"dash\":\"solid\"}, name=\"Pred Untransformed\", fill=\"tonexty\"))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[0, :, 1], line={\"color\": col_1, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[2, :, 1], line={\"color\": col_1, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[4, :, 1], line={\"color\": col_1, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[1, :, 1], line={\"color\": col_1, \"dash\":\"solid\"}, showlegend=False))\n",
    "fig.add_trace(go.Scatter(x=x_pred.flatten(), y=quantiles[3, :, 1], line={\"color\": col_1, \"dash\":\"solid\"}, name=\"Pred Transformed\", fill=\"tonexty\"))\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of observations is limited and the true volatility of observed values, in particular for higher past volatilities is likely to be understated. While predictions on transformed data likely underpredict future volatilities if the past was marked by really low volatilities, predictions on transformed data seem to give more credible results for higher past volatility regimes. While in practice getting this exactly right (and experimenting much more with proper predictions based on more than just one input variable would be required), I will leave it at that for now and trust the book for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatility growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of the following i disregards the days of the week, holidays etc. and treat the data as a steady stream of trading days. While not entirely accurate this seems somewhat justified from the analysis above and common practice (cf. Hull).\n",
    "\n",
    "Let $N$ be the number of observed trading days, $\\{x_0, ..., x_{N-1}\\}$ be the observed log returns, $w \\in \\mathbb{N}_+$ the window size, and $t \\in \\{w, 1, ..., N-w-1\\}$ be a time point at which the volatility is observed. Let $\\hat{\\sigma}_{t}^{w} := \\sqrt{\\frac{1}{w} \\cdot \\sum_{i=t-w+1}^{t}(x_i - \\bar{x}_t)^2}$ with $\\bar{x}_t := \\frac{1}{w} \\cdot \\sum_{i=t-w+1}^{t}x_i$.\n",
    "\n",
    "Assuming the daily log returns follow a zero centred normal distribution with standard deviation $\\hat{\\sigma}_{t}^{w}$, I can normalise these forward returns to make them all standard normal and hence comparable. The expectation is then that $Y_{t, j}:=\\sum_{k=1}^{j}x_{n_t + k} \\sim \\mathcal{N(0, j)}$. To verify this, I will have to choose the $w$ and the $t$ such that the sample size is large enough (small $w$) but the $t$ are far enough apart such that the dependence is not too bad.\n",
    "\n",
    "Before having done the analysis my expectation is that the lower tail of the distribution is heavier than the upper tail (big moves tend to be to the downside), and that it is leptokurtic (movements are flat followed by larger movements rather than a steady creep upwards). \n",
    "\n",
    "I will test different sizes for $w$, but have the windows overlap, such that the evaluation period of one $t$ is the data on which the standard deviation of the next window is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observed_returns(w, correct: bool = True) -> np.ndarray:\n",
    "    \"\"\"Gets all valid cumulative log returns\"\"\"\n",
    "    returns = spx_hist[\"log_return\"]\n",
    "    if correct:\n",
    "        returns = returns - returns.mean()\n",
    "    backward = returns.rolling(w).std().values\n",
    "    forward = np.concatenate([\n",
    "        ((returns\n",
    "          .rolling(pd.api.indexers.FixedForwardWindowIndexer(window_size=n))\n",
    "          .sum()) \n",
    "         - returns)\n",
    "        .values\n",
    "        .reshape([-1, 1])\n",
    "        for n in range(2, w+2)],\n",
    "        axis=1)\n",
    "    forward_norm = forward / np.tile(backward.reshape([-1, 1]), (1, w))\n",
    "    forward_norm = forward_norm[~np.isnan(forward_norm).any(axis=1)]\n",
    "    return forward_norm\n",
    "\n",
    "def select_observed_returns(forward_norm: np.ndarray, w: int, offset=0) -> np.ndarray:\n",
    "    \"\"\"Selects log returns so that they become less correlated\"\"\"\n",
    "    return forward_norm[offset::w, :]\n",
    "\n",
    "def get_quantiles(forward_norm: np.ndarray, quantiles: list) -> np.ndarray:\n",
    "    \"\"\"Calculates quantiles from the observed returns (to be compared with the normal quantiles)\"\"\"\n",
    "    return np.quantile(forward_norm, quantiles, axis=0)\n",
    "\n",
    "def get_window_quantiles(w: int, offset=0, correct: bool = True, quantiles=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]):\n",
    "    cum_returns = get_observed_returns(w, correct)\n",
    "    cum_norm_returns = select_observed_returns(cum_returns, w, offset=offset)\n",
    "    return get_quantiles(cum_norm_returns, quantiles), cum_norm_returns.shape[0]\n",
    "\n",
    "def get_normal_quantiles(t_max: int, quantiles: list = [0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]) -> np.ndarray:\n",
    "    \"\"\"Returns theoretical quantiles from the standard normal\"\"\"\n",
    "    q = norm.ppf(quantiles).reshape([-1, 1])\n",
    "    scale = np.array([np.sqrt(i + 1) for i in range(t_max)]).reshape([1, -1])\n",
    "    return np.matmul(q, scale)\n",
    "\n",
    "def add_traces(fig, quantiles: np.ndarray, col: str, fillcol: str, name: str):\n",
    "    \"\"\"Adds quantile traces to the fig and returns the fig. Assumes there are 7 quantiles to show with 2-4 in colors\"\"\"\n",
    "    fig.add_trace(go.Scatter(x=[i + 1 for i in range(quantiles.shape[1])], y=quantiles[0, :], line={\"color\": col, \"dash\":\"dot\"}, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[i + 1 for i in range(quantiles.shape[1])], y=quantiles[6, :], line={\"color\": col, \"dash\":\"dot\"}, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[i + 1 for i in range(quantiles.shape[1])], y=quantiles[1, :], line={\"color\": col, \"dash\":\"dash\"}, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[i + 1 for i in range(quantiles.shape[1])], y=quantiles[5, :], line={\"color\": col, \"dash\":\"dash\"}, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[i + 1 for i in range(quantiles.shape[1])], y=quantiles[3, :], line={\"color\": col, \"dash\":\"solid\"}, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[i + 1 for i in range(quantiles.shape[1])], y=quantiles[2, :], line={\"color\": \"rgba(0, 0, 0, 0)\", \"dash\":\"solid\"}, showlegend=False))\n",
    "    fig.add_trace(go.Scatter(x=[i + 1 for i in range(quantiles.shape[1])], y=quantiles[4, :], line={\"color\": \"rgba(0, 0, 0, 0)\", \"dash\":\"solid\"}, name=name, fill=\"tonexty\", fillcolor=fillcol))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 21*6\n",
    "\n",
    "\n",
    "uncorrected_window_quantiles, _ = get_window_quantiles(w, correct=False)\n",
    "corrected_window_quantiles, n = get_window_quantiles(w)\n",
    "\n",
    "col_0 = \"rgba(31,119,180, 0.6)\"\n",
    "col_0_f = \"rgba(31,119,180, 0.3)\"\n",
    "col_1 = \"rgba(255,127,14, 0.8)\"\n",
    "col_1_f = \"rgba(255,127,14, 0.4)\"\n",
    "gray = \"rgba(90, 90, 90, 0.8)\"\n",
    "gray_f = \"rgba(90, 90, 90, 0.4)\"\n",
    "\n",
    "\n",
    "fig = go.Figure(layout_title=f\"True development too positive, smaller IQR and unexpected tails, w={w}, n={n}\",\n",
    "                layout_xaxis_title=\"Trading days after t\",\n",
    "                layout_yaxis_title=\"Cumulative normalised return\")\n",
    "\n",
    "fig = add_traces(fig, get_normal_quantiles(w), gray, gray_f, \"Normal\")\n",
    "fig = add_traces(fig, uncorrected_window_quantiles, col_0, col_0_f, \"Observed Uncorrected\")\n",
    "fig = add_traces(fig, corrected_window_quantiles, col_1, col_1_f, \"Observed Corrected\")\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectations were partly met. First, it has to be noted that there is a central trend in the returns (after all, we expect stocks to have positive returns over the long run) which explains why there uncorrected returns deviate from the zero centred normal assumption by rising over time. I therefore added a correction term and subtracted mean and median (here only the mean is shown). After this correction the central tendency is a surprisingly good fit for the normal when it comes to the quartiles.\n",
    "\n",
    "That said, the tails do not seem to follow the normal assumption. As expected the lower tails are heavier. However, while the uncorrected observations have an upper tails that looks about right, the distance between the median and that upper tail is too narrow, as evidenced by the too light tails of the corrected graph. Still, with such a limited sample size it is hard to make statements about the tails.\n",
    "\n",
    "I tested several different window sizes and for all of them the above holds. Also I tested accepting the correlated measures of performing the calculations on every calendar day instead of in intervals of $w$ and got qualitatively similar resuts. I was surprised at their robustnes.\n",
    "\n",
    "Of course it has to be said that SPX is an extremely liquid index and that individual firms can be expected to show very different behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Implied volatilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
